{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T20:20:28.021171Z",
     "start_time": "2025-02-14T20:20:27.579238Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "\"\"\"\n",
    "3_model_training.py\n",
    "-------------------\n",
    "用途:\n",
    "1. 读取 2_feature_extraction.py 生成的 feature_index.csv\n",
    "2. 构建 BiLSTMChorus (示例) 或简化 MLP, 并使用 Weighted BCE Loss\n",
    "3. 训练后保存 model.pt\n",
    "\n",
    "运行:\n",
    "  python 3_model_training.py\n",
    "\n",
    "输出:\n",
    "  ../saved_models/bilstm_chorus_model.pt\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "FEATURE_FOLDER = \"../data/processed/\"\n",
    "MODEL_PATH = \"../saved_models/bilstm_chorus_model.pt\"\n",
    "\n",
    "class BiLSTMChorus(nn.Module):\n",
    "    def __init__(self, input_dim=141, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 1)  # 二分类 => 输出一个logit\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape => (batch, seq_len=1, input_dim)\n",
    "        lstm_out, _ = self.lstm(x)  # (batch, 1, hidden_dim*2)\n",
    "        out = self.fc(lstm_out[:, -1, :])  # (batch, 1)\n",
    "        return out\n",
    "\n",
    "def main():\n",
    "    df = pd.read_csv(os.path.join(FEATURE_FOLDER, \"feature_index.csv\"))\n",
    "    print(\"读取 feature_index.csv =>\")\n",
    "    print(df.head())\n",
    "\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        feat_path = row[\"feature_path\"]\n",
    "        feats = np.load(feat_path)\n",
    "        label = row[\"label\"]\n",
    "        features_list.append(feats)\n",
    "        labels_list.append(label)\n",
    "\n",
    "    X = np.array(features_list, dtype=np.float32)  # shape=(N, 3*feature_dim)\n",
    "    y = np.array(labels_list, dtype=np.int64)\n",
    "\n",
    "    print(\"X shape=\", X.shape, \", y shape=\", y.shape)\n",
    "    print(\"正例数=\", sum(y == 1), \", 负例数=\", sum(y == 0))\n",
    "\n",
    "    # 拆分训练/验证集\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    print(\"Train size=\", X_train.shape, \"Val size=\", X_val.shape)\n",
    "\n",
    "    input_dim = X.shape[1]  # 例如 141\n",
    "    hidden_dim = 64\n",
    "    model = BiLSTMChorus(input_dim, hidden_dim)\n",
    "    print(model)\n",
    "\n",
    "    # Weighted BCEWithLogitsLoss (提高对副歌(正例)的召回)\n",
    "    pos_count = sum(y_train == 1)\n",
    "    neg_count = sum(y_train == 0)\n",
    "    if pos_count == 0:\n",
    "        pos_weight_val = 1.0\n",
    "    else:\n",
    "        pos_weight_val = float(neg_count) / float(pos_count)\n",
    "    pos_weight_tensor = torch.tensor([pos_weight_val], dtype=torch.float32)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    X_train_t = torch.from_numpy(X_train)\n",
    "    y_train_t = torch.from_numpy(y_train).float()  # 用于BCE, 故转为float\n",
    "    X_val_t = torch.from_numpy(X_val)\n",
    "    y_val_t = torch.from_numpy(y_val).float()\n",
    "\n",
    "    def train_one_epoch(model, X_data, y_data, batch_size=32):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        indices = np.arange(len(X_data))\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        for start_idx in range(0, len(X_data), batch_size):\n",
    "            end_idx = start_idx + batch_size\n",
    "            batch_idx = indices[start_idx:end_idx]\n",
    "\n",
    "            x_batch = X_data[batch_idx].unsqueeze(1)  # (b,1,input_dim)\n",
    "            y_batch = y_data[batch_idx].unsqueeze(1)  # (b,1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x_batch)       # shape=(b,1)\n",
    "            loss = criterion(logits, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / (len(X_data) / batch_size)\n",
    "        return avg_loss\n",
    "\n",
    "    def evaluate(model, X_data, y_data, batch_size=32):\n",
    "        model.eval()\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            indices = np.arange(len(X_data))\n",
    "            for start_idx in range(0, len(X_data), batch_size):\n",
    "                end_idx = start_idx + batch_size\n",
    "                batch_idx = indices[start_idx:end_idx]\n",
    "\n",
    "                x_batch = X_data[batch_idx].unsqueeze(1)\n",
    "                y_batch = y_data[batch_idx].unsqueeze(1)\n",
    "\n",
    "                logits = model(x_batch)\n",
    "                loss = criterion(logits, y_batch)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # 预测: sigmoid>0.5 => 1\n",
    "                pred = (torch.sigmoid(logits) > 0.5).float()\n",
    "                correct += (pred == y_batch).sum().item()\n",
    "                total += y_batch.numel()\n",
    "\n",
    "        avg_loss = total_loss / (len(X_data) / batch_size)\n",
    "        acc = correct / total\n",
    "        return avg_loss, acc\n",
    "\n",
    "    epochs = 30\n",
    "    best_val_acc = 0.0\n",
    "    for ep in range(1, epochs + 1):\n",
    "        tr_loss = train_one_epoch(model, X_train_t, y_train_t, batch_size=32)\n",
    "        val_loss, val_acc = evaluate(model, X_val_t, y_val_t, batch_size=32)\n",
    "\n",
    "        # 若val_acc更好, 保存模型\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "\n",
    "        print(f\"Epoch {ep} => train_loss={tr_loss:.4f}, val_loss={val_loss:.4f}, val_acc={val_acc:.4f}\")\n",
    "\n",
    "    print(\"最优 val_acc=\", best_val_acc)\n",
    "    print(\"已保存模型 =>\", MODEL_PATH)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "e699125cae1770ca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取 feature_index.csv =>\n",
      "                                        feature_path  label  \\\n",
      "0  ../data/processed/Taylor Swift - Cruel Summer....      0   \n",
      "1  ../data/processed/Taylor Swift - Cruel Summer....      0   \n",
      "2  ../data/processed/Taylor Swift - Cruel Summer....      0   \n",
      "3  ../data/processed/Taylor Swift - Cruel Summer....      0   \n",
      "4  ../data/processed/Taylor Swift - Cruel Summer....      0   \n",
      "\n",
      "                          filename  \n",
      "0  Taylor Swift - Cruel Summer.mp3  \n",
      "1  Taylor Swift - Cruel Summer.mp3  \n",
      "2  Taylor Swift - Cruel Summer.mp3  \n",
      "3  Taylor Swift - Cruel Summer.mp3  \n",
      "4  Taylor Swift - Cruel Summer.mp3  \n",
      "X shape= (467, 141) , y shape= (467,)\n",
      "正例数= 82 , 负例数= 385\n",
      "Train size= (373, 141) Val size= (94, 141)\n",
      "BiLSTMChorus(\n",
      "  (lstm): LSTM(141, 64, batch_first=True, bidirectional=True)\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Epoch 1 => train_loss=1.1376, val_loss=1.1109, val_acc=0.4681\n",
      "Epoch 2 => train_loss=1.0694, val_loss=1.0587, val_acc=0.4894\n",
      "Epoch 3 => train_loss=1.0136, val_loss=1.0126, val_acc=0.4894\n",
      "Epoch 4 => train_loss=0.9512, val_loss=0.9727, val_acc=0.4894\n",
      "Epoch 5 => train_loss=0.9005, val_loss=0.9369, val_acc=0.5000\n",
      "Epoch 6 => train_loss=0.8359, val_loss=0.9079, val_acc=0.5106\n",
      "Epoch 7 => train_loss=0.7931, val_loss=0.8799, val_acc=0.5319\n",
      "Epoch 8 => train_loss=0.7444, val_loss=0.8514, val_acc=0.5532\n",
      "Epoch 9 => train_loss=0.6907, val_loss=0.8268, val_acc=0.5745\n",
      "Epoch 10 => train_loss=0.6510, val_loss=0.8013, val_acc=0.6170\n",
      "Epoch 11 => train_loss=0.6108, val_loss=0.7766, val_acc=0.6702\n",
      "Epoch 12 => train_loss=0.5699, val_loss=0.7468, val_acc=0.7021\n",
      "Epoch 13 => train_loss=0.5275, val_loss=0.7182, val_acc=0.7021\n",
      "Epoch 14 => train_loss=0.4912, val_loss=0.6904, val_acc=0.7234\n",
      "Epoch 15 => train_loss=0.4490, val_loss=0.6631, val_acc=0.7766\n",
      "Epoch 16 => train_loss=0.4192, val_loss=0.6420, val_acc=0.7872\n",
      "Epoch 17 => train_loss=0.3923, val_loss=0.6274, val_acc=0.8404\n",
      "Epoch 18 => train_loss=0.3640, val_loss=0.6079, val_acc=0.8298\n",
      "Epoch 19 => train_loss=0.3374, val_loss=0.5875, val_acc=0.8511\n",
      "Epoch 20 => train_loss=0.3171, val_loss=0.5753, val_acc=0.8511\n",
      "Epoch 21 => train_loss=0.3076, val_loss=0.5726, val_acc=0.8617\n",
      "Epoch 22 => train_loss=0.2797, val_loss=0.5718, val_acc=0.8617\n",
      "Epoch 23 => train_loss=0.2564, val_loss=0.5742, val_acc=0.8723\n",
      "Epoch 24 => train_loss=0.2431, val_loss=0.5662, val_acc=0.8723\n",
      "Epoch 25 => train_loss=0.2356, val_loss=0.5606, val_acc=0.8723\n",
      "Epoch 26 => train_loss=0.2204, val_loss=0.5555, val_acc=0.8723\n",
      "Epoch 27 => train_loss=0.2117, val_loss=0.5606, val_acc=0.8723\n",
      "Epoch 28 => train_loss=0.2033, val_loss=0.5587, val_acc=0.8723\n",
      "Epoch 29 => train_loss=0.1895, val_loss=0.5659, val_acc=0.8723\n",
      "Epoch 30 => train_loss=0.1778, val_loss=0.5740, val_acc=0.8830\n",
      "最优 val_acc= 0.8829787234042553\n",
      "已保存模型 => ../saved_models/bilstm_chorus_model.pt\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T18:27:21.059884Z",
     "start_time": "2025-02-14T18:27:21.058307Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "16b4bd8b42aafde5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T21:07:48.652639Z",
     "start_time": "2025-02-12T21:07:48.650684Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "ff78eb4b5f20574d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "aa8741744d0c3673"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
